# -*- coding: utf-8 -*-
"""Transfer learning with Empirical eqn

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mohammadtawsif/transfer-learning-with-empirical-eqn.3bdb085c-31a0-4fa7-bdd2-f90f86c106fb.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250405/auto/storage/goog4_request%26X-Goog-Date%3D20250405T074606Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34685411c514e86916c4b27529d775d7ec64f673468f9b143df3ab49c8a6b03b66e369fd7f7897043a2537427489663a576760f8a3da8b9e7c159e07b3126750ae4bf40f1bba6f9a67cdac01f096315ff327d94fecd6a13fa89b5e48907aa7d52c81a470ff221c0fb30cdd725eca24c66fdd0f4e50b2e9a82e24c0927de41c504253e1378423384eff1989ccea012159c7b4f7018b17cd5e617dff5b98038d4b8b06683dc8cb5123a1e0c62abded5d2704921d8b7fff199825b2fe5d4474f419876a9218c54e90b0023f8daad749bb0d2c173c3278ef823860ca169a9ad73e20d1359926a4a52621098f6db886a4415d0a72a2a1450ac16a8865c1cbbb0a154d
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

mohammadtawsif_tawsiff_path = kagglehub.dataset_download('mohammadtawsif/tawsiff')

print('Data source import complete.')

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate, Flatten, Dropout, BatchNormalization, Multiply, LayerNormalization
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from scipy.sparse import csr_matrix
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.optimizers.schedules import CosineDecay
from kerastuner.tuners import RandomSearch
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt

# Load tabular data
tabular_data = pd.read_csv('/kaggle/input/tawsiff/tabulardata1.csv')  # Update path as needed

# Preprocessing Tabular Data
tabular_features = tabular_data.drop(columns=["faceImage"])
tabular_labels = tabular_data["Age(years)"]

# Normalize age labels
label_scaler = StandardScaler()
y_tabular_scaled = label_scaler.fit_transform(tabular_labels.values.reshape(-1, 1)).flatten()

# Extract features and labels
X_tabular = tabular_features.drop(columns=["Age(years)"])
y_tabular = y_tabular_scaled  # Use scaled labels

# Handle categorical and numerical features
categorical_features = ["Blood Pressure (s/d)"]
numerical_features = [col for col in X_tabular.columns if col not in categorical_features]

# Preprocessing pipelines
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown="ignore", sparse_output=False)

# Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ("num", numerical_transformer, numerical_features),
        ("cat", categorical_transformer, categorical_features)
    ],
    sparse_threshold=0
)

# Preprocess tabular data
X_tabular_preprocessed = preprocessor.fit_transform(X_tabular)

# Convert sparse to dense if needed
if isinstance(X_tabular_preprocessed, csr_matrix):
    X_tabular_preprocessed = X_tabular_preprocessed.toarray()

# Image IDs
image_ids = tabular_data["faceImage"]

# Split data
X_tabular_train, X_tabular_test, y_train, y_test, image_ids_train, image_ids_test = train_test_split(
    X_tabular_preprocessed, y_tabular, image_ids, test_size=0.2, random_state=42
)

# Image preprocessing
image_data_path = '/kaggle/input/tawsiff/imagedata/imagedata/'
image_size = (128, 128)
batch_size = 4

train_image_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode='nearest'
)

test_image_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_image_generator = train_image_datagen.flow_from_dataframe(
    pd.DataFrame({'filename': image_ids_train.apply(lambda x: f'{x}.jpg')}),
    directory=image_data_path,
    x_col='filename',
    y_col=None,
    target_size=image_size,
    class_mode=None,
    batch_size=batch_size,
    shuffle=False
)

test_image_generator = test_image_datagen.flow_from_dataframe(
    pd.DataFrame({'filename': image_ids_test.apply(lambda x: f'{x}.jpg')}),
    directory=image_data_path,
    x_col='filename',
    y_col=None,
    target_size=image_size,
    class_mode=None,
    batch_size=batch_size,
    shuffle=False
)

# Align images and tabular data
def create_tf_dataset(image_gen, tabular_data, labels, batch_size):
    images = []
    valid_indices = []
    for i in range(len(image_gen)):
        batch = image_gen[i]
        batch_size_actual = batch.shape[0]
        start_idx = i * image_gen.batch_size
        end_idx = start_idx + batch_size_actual
        if end_idx > len(tabular_data):
            batch = batch[:len(tabular_data) - start_idx]
            images.append(batch)
            valid_indices.extend(range(start_idx, start_idx + batch.shape[0]))
            break
        images.append(batch)
        valid_indices.extend(range(start_idx, end_idx))

    images = np.concatenate(images, axis=0)
    tabular_data = tabular_data[valid_indices]
    labels = labels[valid_indices]

    dataset = tf.data.Dataset.from_tensor_slices((
        {
            'image_input': images,
            'tabular_input': tabular_data.astype(np.float32)
        },
        labels.astype(np.float32)
    ))
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Create datasets
train_dataset = create_tf_dataset(train_image_generator, X_tabular_train, y_train, batch_size)
test_dataset = create_tf_dataset(test_image_generator, X_tabular_test, y_test, batch_size)

# Transfer Learning with EfficientNetB0
def build_model(hp):
    # Image model (EfficientNetB0)
    image_input = Input(shape=(*image_size, 3), name="image_input")
    base_model = EfficientNetB0(include_top=False, weights="imagenet", input_tensor=image_input, pooling="avg")
    base_model.trainable = False  # Freeze the base model initially
    image_output = base_model.output
    image_output = Dense(64, activation="relu", name="image_output")(image_output)

    # Tabular model (Deeper)
    tabular_input = Input(shape=(X_tabular_preprocessed.shape[1],), name="tabular_input")
    y = Dense(
        units=hp.Int('tabular_units_1', min_value=128, max_value=512, step=128),
        activation="relu",
        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))
    )(tabular_input)
    y = LayerNormalization()(y)
    y = Dropout(hp.Float('dropout_tabular_1', min_value=0.3, max_value=0.6))(y)
    y = Dense(
        units=hp.Int('tabular_units_2', min_value=64, max_value=256, step=64),
        activation="relu",
        kernel_regularizer=l2(hp.Float('l2_reg', min_value=1e-4, max_value=5e-2, sampling='log'))
    )(y)
    y = LayerNormalization()(y)
    tabular_output = Dense(64, activation="relu", name="tabular_output")(y)

    # Empirical Equation: Weighted Average of Image and Tabular Predictions
    image_pred = Dense(1, activation="linear", name="image_pred")(image_output)
    tabular_pred = Dense(1, activation="linear", name="tabular_pred")(tabular_output)
    combined_pred = Multiply()([image_pred, tabular_pred])  # Element-wise multiplication
    final_output = Dense(1, activation="linear", name="final_output")(combined_pred)

    # Compile with AdamW and Cosine Decay
    lr_schedule = CosineDecay(
        initial_learning_rate=hp.Float('learning_rate', min_value=1e-5, max_value=1e-3, sampling='log'),
        decay_steps=10000
    )
    model = Model(inputs=[image_input, tabular_input], outputs=final_output)
    model.compile(
        optimizer=AdamW(learning_rate=lr_schedule, weight_decay=1e-4, clipnorm=1.0),
        loss="mse",
        metrics=["mae"]
    )
    return model

# Hyperparameter tuning
tuner = RandomSearch(
    build_model,
    objective='val_mae',
    max_trials=10,
    executions_per_trial=2,
    directory='tuner_results',
    project_name='efficientnet_low_mae'
)

# Callbacks
early_stopping = EarlyStopping(monitor='val_mae', patience=50, restore_best_weights=True, mode='min')

# Perform tuning
tuner.search(
    train_dataset,
    validation_data=test_dataset,
    epochs=75,
    callbacks=[early_stopping]
)

# Get best model
best_model = tuner.get_best_models(num_models=1)[0]

# Evaluate (MAE in scaled space)
loss, mae_scaled = best_model.evaluate(test_dataset)
print(f"Test Loss (Scaled): {loss}, Test MAE (Scaled): {mae_scaled}")

# Convert MAE back to original scale
mae_original = label_scaler.inverse_transform([[mae_scaled]])[0][0] - label_scaler.inverse_transform([[0]])[0][0]
print(f"Test MAE (Original Scale): {mae_original}")

# Final training
history = best_model.fit(
    train_dataset,
    validation_data=test_dataset,
    epochs=100,
    callbacks=[early_stopping]
)

# Plot training history
def plot_training_history(history):
    epochs = range(1, len(history.history['loss']) + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')
    plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss (MSE)')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history.history['mae'], label='Training MAE', color='green')
    plt.plot(epochs, history.history['val_mae'], label='Validation MAE', color='red')
    plt.title('Training and Validation MAE (Scaled)')
    plt.xlabel('Epochs')
    plt.ylabel('Mean Absolute Error (MAE)')
    plt.legend()

    plt.tight_layout()
    plt.show()

plot_training_history(history)

# Residual analysis (in original scale)
predictions_scaled = best_model.predict(test_dataset)
predictions = label_scaler.inverse_transform(predictions_scaled)
y_test_original = label_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()
residuals = y_test_original - predictions.flatten()
plt.figure(figsize=(8, 6))
plt.scatter(y_test_original, residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('True Age')
plt.ylabel('Residual (True - Predicted)')
plt.title('Residual Plot (Original Scale)')
plt.show()

# Prediction function with Empirical Equation
def predict_age(image_path, height_cm, weight_kg, bmi, blood_pressure, blood_oxygen, blood_sugar, model, preprocessor, label_scaler, image_size=(128, 128)):
    """
    Predict age using an image and biomarkers with an empirical equation.
    """
    # Preprocess the image
    img = load_img(image_path, target_size=image_size)
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)

    # Preprocess the tabular data
    biomarkers = {
        'Height (cm)': height_cm,
        'Weight (kg)': weight_kg,
        'BMI': bmi,
        'Blood Pressure (s/d)': blood_pressure,
        'Blood Oxygen': blood_oxygen,
        'Blood Sugar(mg/dl)': blood_sugar
    }
    tabular_df = pd.DataFrame([biomarkers])
    for col in X_tabular.columns:
        if col not in tabular_df.columns:
            if col in numerical_features:
                tabular_df[col] = X_tabular[col].median()  # Use median for numerical
            elif col in categorical_features:
                tabular_df[col] = X_tabular[col].mode()[0]  # Use mode for categorical
    tabular_df = tabular_df[X_tabular.columns]
    tabular_processed = preprocessor.transform(tabular_df)
    if isinstance(tabular_processed, csr_matrix):
        tabular_processed = tabular_processed.toarray()
    if len(tabular_processed.shape) == 1:
        tabular_processed = np.expand_dims(tabular_processed, axis=0)

    # Make prediction
    inputs = {
        'image_input': img_array,
        'tabular_input': tabular_processed.astype(np.float32)
    }
    prediction_scaled = model.predict(inputs)[0][0]
    predicted_age = label_scaler.inverse_transform([[prediction_scaled]])[0][0]

    return predicted_age

# Example usage of predict_age
example_image_path = '/kaggle/input/tawsiff/imagedata/imagedata/1005.jpg'
example_height_cm = 160.02
example_weight_kg = 48
example_bmi = 18.74
example_blood_pressure = '116/71'
example_blood_oxygen = 96.0
example_blood_sugar = 97.2

predicted_age = predict_age(
    image_path=example_image_path,
    height_cm=example_height_cm,
    weight_kg=example_weight_kg,
    bmi=example_bmi,
    blood_pressure=example_blood_pressure,
    blood_oxygen=example_blood_oxygen,
    blood_sugar=example_blood_sugar,
    model=best_model,
    preprocessor=preprocessor,
    label_scaler=label_scaler
)

print(f"Predicted Age: {predicted_age:.2f} years")

import joblib

# Save model
best_model.save('age_predictor.h5')

# Save preprocessors
joblib.dump(preprocessor, 'preprocessor.pkl')
joblib.dump(label_scaler, 'label_scaler.pkl')